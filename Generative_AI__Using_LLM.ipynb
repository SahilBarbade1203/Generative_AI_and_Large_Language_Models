{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sdmr12012003/Neural_Networks_and_Large_Language_Models/blob/main/Generative_AI__Using_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NrBt2htxgQI"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUwy2eq-xlNh"
      },
      "outputs": [],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KEhgaVSvx04z",
        "outputId": "e74bb13a-45c4-4cf1-bd12-4c5d7d98688a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.31.0)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchdata) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchdata) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdhyoOyTx7Kz"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5s2lkKox_B8"
      },
      "outputs": [],
      "source": [
        "tokenizer1 = AutoTokenizer.from_pretrained(\"Salesforce/codet5-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2mfcjb5yDO7"
      },
      "outputs": [],
      "source": [
        "example = [\"Do not meddle in the affairs of wizards,\",\"for they are subtle and quick to anger.\"]\n",
        "encoded_input = tokenizer(example,padding = True, return_tensors = 'pt')\n",
        "print(encoded_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9QRDXBByGYG"
      },
      "outputs": [],
      "source": [
        "encoded_input1 = tokenizer1(example,padding = True, return_tensors = 'pt')\n",
        "print(encoded_input1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahjlkaY7yJad"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig\n",
        "details = AutoConfig.from_pretrained(\"google/flan-t5-base\")\n",
        "details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vn3uXiZPyUm8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "model = AutoModel.from_pretrained('google/flan-t5-base')\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dgXV6W8yXvM"
      },
      "outputs": [],
      "source": [
        "okk = AutoConfig.from_pretrained('google/flan-t5-base')\n",
        "okk1 = AutoModel.from_config(okk)\n",
        "okk1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp4sPGnFybit"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForPreTraining\n",
        "pre_okk = AutoModelForPreTraining.from_config(okk)\n",
        "pre_okk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVY-BSpIyqlv"
      },
      "outputs": [],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4LgSVCDytZT"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset('knkarthick/dialogsum')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY0gqWZ6yw4s"
      },
      "outputs": [],
      "source": [
        "print(data['train'][100]['dialogue'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ty0SAcQoyz1Z"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "models = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
        "models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baRlz7Ery2Im"
      },
      "outputs": [],
      "source": [
        "decoded_input = tokenizer1.decode(encoded_input1['input_ids'][1], skip_special_tokens = True)\n",
        "decoded_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VABShc5iy47M"
      },
      "outputs": [],
      "source": [
        "d = [40,200]\n",
        "for i,index in enumerate(d):\n",
        "  dialogue = data['test'][index]['dialogue']\n",
        "  summary = data['test'][index]['summary']\n",
        "\n",
        "  inputs = tokenizer(dialogue, return_tensors = 'pt')\n",
        "  output = tokenizer.decode(models.generate(inputs['input_ids'], max_new_tokens = 50)[0], skip_special_tokens = True)\n",
        "\n",
        "  print(summary)\n",
        "  print(output)\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PTqAbzVy8eA"
      },
      "outputs": [],
      "source": [
        "for i,index in enumerate(d):\n",
        "  dialogue = data['test'][index]['dialogue']\n",
        "  summary = data['test'][index]['summary']\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "\n",
        "  Summarize the following conversation:\n",
        "  {dialogue}\n",
        "\n",
        "  Summary:\n",
        "\n",
        "   \"\"\"\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors = 'pt')\n",
        "  output = tokenizer.decode(models.generate(inputs['input_ids'], max_new_tokens = 50)[0], skip_special_tokens = True)\n",
        "\n",
        "  print(summary)\n",
        "  print(output)\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iin9FBy4zBd3"
      },
      "outputs": [],
      "source": [
        "for i,index in enumerate(d):\n",
        "  dialogue = data['test'][index]['dialogue']\n",
        "  summary = data['test'][index]['summary']\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "\n",
        "  Summarize the following conversation:\n",
        "  {dialogue}\n",
        "\n",
        "  What is going on?\n",
        "\n",
        "   \"\"\"\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors = 'pt')\n",
        "  output = tokenizer.decode(models.generate(inputs['input_ids'], max_new_tokens = 50)[0], skip_special_tokens = True)\n",
        "\n",
        "  print(summary)\n",
        "  print(output)\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OORgOHCgzdDc"
      },
      "outputs": [],
      "source": [
        "def prompt(example_full_index,example_summarize_index):\n",
        "  prompt = ''\n",
        "  for i,index in enumerate(example_full_index):\n",
        "    dialogue = data['test'][index]['dialogue']\n",
        "    summary = data['test'][index]['summary']\n",
        "\n",
        "    prompt += f\"\"\"\n",
        "\n",
        "    Summarize the following conversation:\n",
        "    {dialogue}\n",
        "\n",
        "    What is going on?\n",
        "    {summary}\n",
        "\n",
        "    \"\"\"\n",
        "  dialogue1 = data['test'][example_summarize_index]['dialogue']\n",
        "\n",
        "  prompt += f\"\"\"\n",
        "\n",
        "  Summarize the following conversation:\n",
        "  {dialogue1}\n",
        "\n",
        "  What is going on?\n",
        "  \"\"\"\n",
        "\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oKS6dwOzgkU"
      },
      "outputs": [],
      "source": [
        "example_full_index = [40]\n",
        "example_summarize_index = 200\n",
        "\n",
        "one_shot_prompt = prompt(example_full_index,example_summarize_index)\n",
        "one_shot_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28u9aEU6zjlk"
      },
      "outputs": [],
      "source": [
        "input = tokenizer(one_shot_prompt, return_tensors = 'pt')\n",
        "output = tokenizer.decode(models.generate(input['input_ids'] , max_new_tokens = 50)[0], skip_special_tokens = True)\n",
        "summary = data['test'][example_summarize_index]['summary']\n",
        "print(summary)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaVXt13Tzm6p"
      },
      "outputs": [],
      "source": [
        "example_full_index = [40,80,100]\n",
        "example_summarize_index = 200\n",
        "\n",
        "few_shot_prompt = prompt(example_full_index,example_summarize_index)\n",
        "few_shot_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VB0MRkC1ZHo"
      },
      "outputs": [],
      "source": [
        "input = tokenizer(few_shot_prompt, return_tensors = 'pt')\n",
        "output = tokenizer.decode(models.generate(input['input_ids'] , max_new_tokens = 50)[0], skip_special_tokens = True)\n",
        "summary = data['test'][example_summarize_index]['summary']\n",
        "print(summary)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeChwU6T1cqd"
      },
      "outputs": [],
      "source": [
        "from transformers import GenerationConfig\n",
        "\n",
        "generation_config = GenerationConfig(max_new_tokens = 50)\n",
        "generation_config1 = GenerationConfig(max_new_tokens = 50, do_sample = True , temperature = 0.1)\n",
        "generation_config2 = GenerationConfig(max_new_tokens = 50, do_sample = True , temperature = 0.5)\n",
        "generation_config3 = GenerationConfig(max_new_tokens = 50, do_sample = True , temperature = 1.0)\n",
        "\n",
        "input = tokenizer(few_shot_prompt, return_tensors = 'pt')\n",
        "output = tokenizer.decode(models.generate(input['input_ids'] , generation_config = generation_config)[0], skip_special_tokens = True)\n",
        "summary = data['test'][example_summarize_index]['summary']\n",
        "print(summary)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2z4nuuA1hHK"
      },
      "outputs": [],
      "source": [
        "input = tokenizer(few_shot_prompt, return_tensors = 'pt')\n",
        "output = tokenizer.decode(models.generate(input['input_ids'] , generation_config = generation_config1)[0], skip_special_tokens = True)\n",
        "summary = data['test'][example_summarize_index]['summary']\n",
        "print(summary)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-oIykn71lED"
      },
      "outputs": [],
      "source": [
        "input = tokenizer(few_shot_prompt, return_tensors = 'pt')\n",
        "output = tokenizer.decode(models.generate(input['input_ids'] , generation_config = generation_config2)[0], skip_special_tokens = True)\n",
        "summary = data['test'][example_summarize_index]['summary']\n",
        "print(summary)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRNax-eG1nMU"
      },
      "outputs": [],
      "source": [
        "input = tokenizer(few_shot_prompt, return_tensors = 'pt')\n",
        "output = tokenizer.decode(models.generate(input['input_ids'] , generation_config = generation_config3)[0], skip_special_tokens = True)\n",
        "summary = data['test'][example_summarize_index]['summary']\n",
        "print(summary)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkxPdHS31ozh"
      },
      "outputs": [],
      "source": [
        "pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv8kee2i1u55"
      },
      "outputs": [],
      "source": [
        "pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbsINxa11x8_"
      },
      "outputs": [],
      "source": [
        "pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqSpWTSR109f"
      },
      "outputs": [],
      "source": [
        "pip install loralib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mlSw52_13zv"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWqfWbiR2FVL"
      },
      "outputs": [],
      "source": [
        "#lets print number of trainable parameter\n",
        "\n",
        "def print_number_of_trainable_parameters(model):\n",
        "  trainable_parameters = 0\n",
        "  all_parameters = 0\n",
        "  for _,param in models.named_parameters():\n",
        "    all_parameters += param.numel()\n",
        "    if param.requires_grad:\n",
        "      trainable_parameters += param.numel()\n",
        "  print(f\"Number of Trainable Parameters : {trainable_parameters} \\nNumber of All Parameters : {all_parameters}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-D9uxFU2JFb"
      },
      "outputs": [],
      "source": [
        "print_number_of_trainable_parameters(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNrKgkzI2NkG"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I45G-PDW2R4-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import argparse\n",
        "import torch.multiprocessing as mp\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributed as dist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxmhcZC22qnW"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bU0dii2y3IWK"
      },
      "outputs": [],
      "source": [
        "models = models.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8Z1TqzI3d7H"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example):\n",
        "    start_prompt = 'Summarize the following conversation:'\n",
        "    end_prompt = '\\n\\nSummary:'\n",
        "\n",
        "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example['dialogue']]\n",
        "    example['input_ids'] = torch.tensor(tokenizer(prompt, padding='max_length', truncation=True, return_tensors='pt').input_ids).to(device).long()\n",
        "    example['label_ids'] = torch.tensor(tokenizer(example['summary'], padding='max_length', truncation=True, return_tensors='pt').input_ids).to(device).long()\n",
        "\n",
        "    print(example['input_ids'].dtype)\n",
        "    return example\n",
        "\n",
        "tokenized_dataset = data.map(tokenize_function, batched=True)\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['id','topic','dialogue','summary'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_vB5icqbKJ5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4D8hfur3tW2"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(tokenized_dataset['train'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4522C3X3zlA"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9IvO7Lf34aD"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(tokenized_dataset['train'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lw_12YBiwbK"
      },
      "outputs": [],
      "source": [
        "# Example to check the format of input_ids in the training dataset\n",
        "training_sample = tokenized_dataset['train'][0]  # Get the first sample from the training set\n",
        "\n",
        "# Access and print the input_ids from the sample\n",
        "input_ids_sample = training_sample['input_ids']\n",
        "print(f\"Data Type of input_ids: {type(input_ids_sample)}\")\n",
        "\n",
        "# Check the length of the list (number of tokens in input_ids)\n",
        "print(f\"Length of input_ids list: {len(input_ids_sample)}\")\n",
        "\n",
        "# Print an example token sequence from the input_ids list\n",
        "print(f\"Example input_ids: {input_ids_sample}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-7evZLFhV9i"
      },
      "outputs": [],
      "source": [
        "pip install accelerate>=0.20.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qopXQeSg38UU"
      },
      "outputs": [],
      "source": [
        "print(f\"Shape of Dataset:\")\n",
        "print(f\"Shape of train dataset : {tokenized_dataset['train'].shape}\")\n",
        "print(f\"Shape of train dataset : {tokenized_dataset['validation'].shape}\")\n",
        "print(f\"Shape of train dataset : {tokenized_dataset['test'].shape}\")\n",
        "\n",
        "print(tokenized_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmVeTZKdcXHL"
      },
      "outputs": [],
      "source": [
        "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = output_dir,\n",
        "    learning_rate = 1e-5,\n",
        "    num_train_epochs = 1,\n",
        "    weight_decay = 0.01,\n",
        "    logging_steps = 1,\n",
        "    max_steps = 1,\n",
        "    per_device_train_batch_size=2,  # Decrease batch size\n",
        "    per_device_eval_batch_size=2,   # Decrease eval batch size if necessary\n",
        "    gradient_accumulation_steps=4  # Accumulate gradients across 4 step\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = models,\n",
        "    args = training_args,\n",
        "    train_dataset = tokenized_dataset['train'],\n",
        "    eval_dataset = tokenized_dataset['validation']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6yXaL3v9o04"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pktZo9Rr4R0_"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzZWUULF4WFt"
      },
      "outputs": [],
      "source": [
        "instruct_model_paths = './instruct_model'\n",
        "\n",
        "trainer.save_model(instruct_model_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5tWvEt7oLr_"
      },
      "outputs": [],
      "source": [
        "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(instruct_model_paths)\n",
        "instruct_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fh4VAWg0sPjo"
      },
      "outputs": [],
      "source": [
        "orignal_model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c43HLXkBoXWs"
      },
      "outputs": [],
      "source": [
        "index = 200\n",
        "\n",
        "dialogue = data['test'][index]['dialogue']\n",
        "human_summary = data['test'][index]['summary']\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following :\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        " \"\"\"\n",
        "\n",
        "input_ids = tokenizer(prompt, truncation = True , return_tensors = 'pt').input_ids\n",
        "orignal_model_output = orignal_model.generate(input_ids, generation_config = GenerationConfig(max_new_tokens = 50,do_sample = True , temperature = 0.1))\n",
        "orignal_model_text = tokenizer.decode(orignal_model_output[0] , skip_special_tokens = True)\n",
        "\n",
        "instruct_model_output = instruct_model.generate(input_ids, generation_config = GenerationConfig(max_new_tokens = 50,do_sample = True , temperature = 0.1))\n",
        "instruct_model_text = tokenizer.decode(instruct_model_output[0] , skip_special_tokens = True)\n",
        "\n",
        "dash_line = \"--------------------------------------\"\n",
        "print(dash_line)\n",
        "print('BASLINE HUMAN SUMMARY\\n')\n",
        "print(human_summary)\n",
        "print(dash_line)\n",
        "print('ORIGNAL_MODEL SUMMARY\\n')\n",
        "print(orignal_model_text)\n",
        "print(dash_line)\n",
        "print('ISTRUCT_FULL_FINE_TUNED MODEL SUMMARY')\n",
        "print(instruct_model_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86Z3bb5Gu3Gj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP00kjgQD8/FM9Tul9JSr/0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}